{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of explaining some of the core processes of text mining, the tool I am going to highlight is the [Natural Language Toolkit](http://www.nltk.org/) (NLTK), a library for processing natural language using the Python scripting language including example corpora. Python was designed to be both readable and extensible. This extensibility is what NLTK is a perfect example is of. After installation and importing, one is able to very quickly perform some very powerful acts of quantitative text analysis. Links and instructions for installing Python and NLTK on your machine are provided at the end of this post, as well as for installing the [Jupyter Notebook](http://jupyter.org/) browser based scripting environment used for writing these scripts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: There isn’t enough space here to explain the basics coding in Python or to explain all of the functions that will be used. There are good introductory resources for coding in Python, such as [CodeAcademy](https://www.codecademy.com/learn/python) which provides interactive tutorials starting with the basics, as well as robust [documentation](https://docs.python.org/3/) for understanding the various functions employed. None of the techniques here are particularly complicated, so you will likely be able to work with this notebook after working through the CodeAcademy tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are resources for getting started:\n",
    "    \n",
    "[Downloading Python](https://wiki.python.org/moin/BeginnersGuide/Download)\n",
    "\n",
    "[Beginner’s Guide to Python](https://wiki.python.org/moin/BeginnersGuide)\n",
    "\n",
    "[Download Anaconda](https://www.continuum.io/downloads) for managing Python versions and add-ons, such as Jupyter (optional)\n",
    "\n",
    "[Install Jupyter Notebook](http://jupyter.readthedocs.io/en/latest/install.html), a browser based environment for script development\n",
    "\n",
    "[Installing NLTK](http://www.nltk.org/install.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependant libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These libraries, which are prepacked sets of modules and functions that provide specific functionality not present in the core Python library, are necessary for running the scripts that follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You will need to install NLTK before you can import it. Instructions for doing so can be found [here](http://www.nltk.org/install.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import urllib.request\n",
    "import nltk\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing text from the web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can import text directly from the full text version of the State Medical Society Journals using the following lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#assign the URL you want to a variable\n",
    "#this is the URL for the plain text version of volume 56 of The West Virginia Medical Journal \n",
    "journalUrl = \"https://archive.org/stream/westvirginiamedi8619west/westvirginiamedi8619west_djvu.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#assign the plain text to a variable as a string\n",
    "journalString = urllib.request.urlopen(journalUrl).read().decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving text to file (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creates directory if it does not exist\n",
    "directory = \"C:/Users/tdahn/Documents/WestVirginiaTXT\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'westvirginiamedi8619west_djvu.txt'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urlEnd = journalUrl.find(\"west/\")\n",
    "journalTitle = journalUrl[(urlEnd + 5):]\n",
    "journalTitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "header = journalString.find('<pre>') + len('<pre>')\n",
    "footer = journalString.find('</pre>')\n",
    "journalString = journalString[header:footer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open(directory + \"/\" + journalTitle, \"w\", encoding='utf-8')\n",
    "f.write(journalString)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading .txt files of journal from a .txt file list of journal URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read .txt file of URLs, file should contain the URL of each journal to be written to file\n",
    "with open(\"C:/Users/tdahn/Desktop/WVurls.txt\") as f:\n",
    "    content = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#removes line break from the end of the URL read into the content variable\n",
    "#note: add a space or additional character to the last URL of the text file to\n",
    "    #avoid removing the last \"t\" from the file extension\n",
    "count = 0\n",
    "for line in content:\n",
    "    line = line[:-1]\n",
    "    content[count] = line\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creates a directory for writing the .txt files\n",
    "directory = \"C:/Users/tdahn/Documents/WestVirginiaTXT\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "#writes files to directory above removing the header and the footer and using the file name from IA for the local filename\n",
    "for line in content:\n",
    "    journalUrl = line\n",
    "    journalString = urllib.request.urlopen(journalUrl).read().decode()\n",
    "    header = journalString.find('<pre>') + len('<pre>')\n",
    "    footer = journalString.find('</pre>')\n",
    "    journalString = journalString[header:footer]\n",
    "    urlEnd = journalUrl.find(\"west/\")\n",
    "    journalTitle = journalUrl[(urlEnd + 5):]\n",
    "    f = open(directory + \"/\" + journalTitle, \"w\", encoding='utf-8')\n",
    "    f.write(journalString)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines clean the Internet Archive header and footer from the raw text string. This is not covered in depth in the post, but is mentioned. Cleaning up texts is always case by case and involves checking your results after each manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove header and footer\n",
    "header = journalString.find('<pre>') + len('<pre>')\n",
    "footer = journalString.find('</pre>')\n",
    "journalString = journalString[header:footer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When digitized images are converted to text using optical character recognition (OCR), the result is most often output as plain text, which contains no formatting. Plain text contains only characters of readable text, and as such a computer cannot initially distinguish linguistic concepts such as punctuation, sentences or even words. As such, an important part of working with texts is to tokenize them, or break them into discreet entities that can be understood as separate units of writing/speech. Assuming we have the plain text of the journal read into a string as above, NLTK allows us to do this easily in one line (the lower function is used to ensure all words are lower case, and as such treated the same by the computer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokenize and make each word lowercase\n",
    "journalTokens = nltk.word_tokenize(journalString.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['•',\n",
       " \"'\",\n",
       " 'bd',\n",
       " '?',\n",
       " '.',\n",
       " '5.05',\n",
       " 'digitized',\n",
       " 'by',\n",
       " 'the',\n",
       " 'internet',\n",
       " 'archive',\n",
       " 'in',\n",
       " '2016',\n",
       " 'https',\n",
       " ':',\n",
       " '//archive.org/details/westvirginiamedi5619west',\n",
       " 'm',\n",
       " 'now',\n",
       " 'ium',\n",
       " '(',\n",
       " '£',\n",
       " ')',\n",
       " '(',\n",
       " 'propionyl',\n",
       " 'erythromycin',\n",
       " 'ester',\n",
       " ',',\n",
       " 'lilly',\n",
       " ')',\n",
       " \"'ey\",\n",
       " 'for',\n",
       " 'chjldre',\n",
       " 'too',\n",
       " '!',\n",
       " 'ilosone',\n",
       " '125',\n",
       " 'suspension',\n",
       " '(',\n",
       " 'propionyl',\n",
       " 'erythromycin',\n",
       " 'ester',\n",
       " 'lauryl',\n",
       " 'sulfate',\n",
       " ',',\n",
       " 'lilly',\n",
       " ')',\n",
       " 'deliciously',\n",
       " 'flavored',\n",
       " 'decisively',\n",
       " 'effective']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the tokens array, first fifty tokens\n",
    "journalTokens[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are many tokens in the array that are not words. The following lines will remove any token that does not contain a character from the alphabet. Further cleanup will be neceassary, for example, the \"digitized by\" stamp is still at the beginning of the text, and words broken by a column margin need to be re-concatenated, but for the sake of demonstration we will consider these results acceptable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "journalWords = [word for word in journalTokens if any([char for char in word if char.isalpha()])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the example above the punctuation is treated as its own token. Many plain text documents need to be cleaned up before being used, including the removing of any header or footer information. Additionally, the removal of non-word tokens such as punctuation may be necessary, as well as the removal of stop words, which are a predetermined set of commonly occurring words deemed unimportant and potentially misleading in natural language processing. Since this process varies from corpus to corpus, it is outside of the scope of this post, but is necessary in order ensure accurate results, and is demonstrated in the code I am making available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load stopwords list from NLTK\n",
    "stopwords = nltk.corpus.stopwords.words(\"English\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove stopwords from journalWords\n",
    "journalStoppedWords = [word for word in journalWords if word not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bd',\n",
       " 'digitized',\n",
       " 'internet',\n",
       " 'archive',\n",
       " 'https',\n",
       " '//archive.org/details/westvirginiamedi5619west',\n",
       " 'ium',\n",
       " 'propionyl',\n",
       " 'erythromycin',\n",
       " 'ester',\n",
       " 'lilly',\n",
       " \"'ey\",\n",
       " 'chjldre',\n",
       " 'ilosone',\n",
       " 'suspension',\n",
       " 'propionyl',\n",
       " 'erythromycin',\n",
       " 'ester',\n",
       " 'lauryl',\n",
       " 'sulfate',\n",
       " 'lilly',\n",
       " 'deliciously',\n",
       " 'flavored',\n",
       " 'decisively',\n",
       " 'effective',\n",
       " 'exceptionally',\n",
       " 'safe',\n",
       " '5-cc',\n",
       " 'teaspoonful',\n",
       " 'provides',\n",
       " 'ilosone',\n",
       " 'lauryl',\n",
       " 'sulfate',\n",
       " 'equivalent',\n",
       " 'mg.',\n",
       " 'erythromycin',\n",
       " 'base',\n",
       " 'activity',\n",
       " 'supplied',\n",
       " 'bottles',\n",
       " 'cc',\n",
       " 'eli',\n",
       " 'lilly',\n",
       " 'company',\n",
       " 'indianapolis',\n",
       " 'indiana',\n",
       " 'u.',\n",
       " 's.',\n",
       " 'i960',\n",
       " 'epilepsy']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the words array, first fifty tokens\n",
    "journalStoppedWords[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Well . . . what next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our text is ready to be worked with, what do we do with it? That question is ultimately dictated by the research goals of the researcher and the corpora available, but I’d like to show how we can very quickly start to draw some conclusions across texts. A very preliminary examination involves the concept of type token ratio, which is simply the ratio of types, unique tokens, and total tokens. We know in our case that a token is a word, and as such a type is a unique word, which means this ratio quickly determines the vocabulary variation of a particular work. There are more complicated considerations, such as the part of speech of a word in order to distinguish between homographs such as bass (fish) versus bass (instrument), but for the sake of this post we will keep it simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type-Token Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create an array of only the unique tokens in the words array\n",
    "journalUniqueWords = set(journalStoppedWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28345"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of unique words\n",
    "len(journalUniqueWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278228"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of words\n",
    "len(journalStoppedWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10187687795620858"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ratio of unique words to total words\n",
    "len(journalUniqueWords)/len(journalStoppedWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This number may not mean much to us by itself, but doing this calculation on a number of texts across a corpus, or even across corpora allows us to start to ask, and maybe answer, some questions. Perhaps we might expect vocabulary size to increase over the 20th century in medical journals due to the increased specialization in the medical profession over that time. Looking at this calculation over time might help us confirm or deny our hypothesis.\n",
    "Another quick and easy, but powerful, analysis that NLTK allows us to do is known as frequency distribution. As a statistical concept, frequency distribution is a tabular display of each outcome in a dataset. For our purposes here it represents the number of times a particular word occurs in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creates a table of the number of occurances of a word in the words array\n",
    "journalFreqTable = nltk.FreqDist(journalStoppedWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medical   m.   d. virginia west  dr. state   j.  may meeting hospital medicine  new association   w. \n",
      "3513 2676 2228 1928 1906 1626 1271 1147 1054 1037 1012  969  949  947  936 \n"
     ]
    }
   ],
   "source": [
    "#display the top twenty results as tabular data\n",
    "journalFreqTable.tabulate(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at Frequency Distributions for multiple word sequences, known as [Ngrams](https://en.wikipedia.org/wiki/N-gram)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a list of all four-grams in our words list\n",
    "#since the stop words are important in phrases, we will us the array that still contatins them\n",
    "fourGrams = list(nltk.ngrams(journalWords, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a frequency distribution of these four grams\n",
    "fourGramsFreqs = nltk.FreqDist(fourGrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create an array of the most common phrases\n",
    "mostCommon = fourGramsFreqs.most_common(40) #change number to decide number of most common to store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('the', 'west', 'virginia', 'medical'), 400),\n",
       " (('west', 'virginia', 'medical', 'journal'), 394),\n",
       " (('of', 'the', 'west', 'virginia'), 351),\n",
       " (('the', 'west', 'virginia', 'state'), 327),\n",
       " (('west', 'virginia', 'state', 'medical'), 324),\n",
       " (('virginia', 'state', 'medical', 'association'), 285),\n",
       " (('a', 'member', 'of', 'the'), 161),\n",
       " (('the', 'american', 'medical', 'association'), 138),\n",
       " (('university', 'school', 'of', 'medicine'), 113),\n",
       " (('annual', 'meeting', 'of', 'the'), 112),\n",
       " (('will', 'be', 'held', 'at'), 103),\n",
       " (('the', 'state', 'medical', 'association'), 101),\n",
       " (('of', 'the', 'state', 'medical'), 97),\n",
       " (('woman’s', 'auxiliary', 'to', 'the'), 96),\n",
       " (('his', 'm.', 'd.', 'degree'), 85),\n",
       " (('the', 'woman’s', 'auxiliary', 'to'), 83),\n",
       " (('of', 'the', 'american', 'medical'), 80),\n",
       " (('m.', 'd.', 'degree', 'from'), 79),\n",
       " (('of', 'the', 'woman’s', 'auxiliary'), 78),\n",
       " (('be', 'held', 'at', 'the'), 78),\n",
       " (('the', 'house', 'of', 'delegates'), 77),\n",
       " (('of', 'the', 'department', 'of'), 74),\n",
       " (('meeting', 'of', 'the', 'west'), 73),\n",
       " (('the', 'west', 'virginia', 'university'), 73),\n",
       " (('received', 'his', 'm.', 'd.'), 72),\n",
       " (('was', 'held', 'at', 'the'), 71),\n",
       " (('per', 'cent', 'of', 'the'), 68),\n",
       " (('state', 'medical', 'association', 'and'), 66),\n",
       " (('at', 'the', 'greenbrier', 'in'), 65),\n",
       " (('virginia', 'university', 'school', 'of'), 64),\n",
       " (('will', 'be', 'held', 'in'), 63),\n",
       " (('west', 'virginia', 'university', 'school'), 63),\n",
       " (('the', 'members', 'of', 'the'), 61),\n",
       " (('to', 'the', 'publication', 'committee'), 61),\n",
       " (('may', 'be', 'obtained', 'by'), 61),\n",
       " (('submitted', 'to', 'the', 'publication'), 61),\n",
       " (('in', 'white', 'sulphur', 'springs'), 60),\n",
       " (('president', 'of', 'the', 'west'), 60),\n",
       " (('the', 'american', 'college', 'of'), 59),\n",
       " (('be', 'obtained', 'by', 'writing'), 59)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mostCommon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that many of the most common four grams here are fairly obvious, and as such not particularly telling, but by carefully preparing your texts and corpora, as well as making decisions about what words should be appended to the stop words list in order to provide more meaningful results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing a corpus from a directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The processes above are all performed on one text for the sake of demonstration. Below the reading of texts from a directory is shown, as well as tokenizing and cleaning those texts. The last step or writing the clean versions to another directory is optional, but helpful in preventing you from having to repeat this process everytime you plan to work.\n",
    "\n",
    "NOTE: In order to use any of the processes above on the corpus level, you will need to us a for loop as well as the dictionary.items() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reads files from a directory into a dictionary element with filename as key and text string as value\n",
    "directory = \"C:/Users/tdahn/Documents/WestVirginiaTXT\"\n",
    "\n",
    "stringDict = {}\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    fileString = os.path.splitext(filename)[0]\n",
    "    text = open(directory + \"/\" + filename, 'r', encoding=\"utf8\")\n",
    "    textString = text.read()\n",
    "    text.close()\n",
    "    stringDict[str(fileString)] = textString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenize texts and save to new dictionary\n",
    "#!!!this will take some time depending on the size or your corpus!!!\n",
    "tokenDict = {}\n",
    "\n",
    "for volume, text in stringDict.items():\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokenDict[volume] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove non-words and save to new dictionary\n",
    "wordsDict = {}\n",
    "\n",
    "for volume, text in tokenDict.items():\n",
    "    textWords = [word for word in text if any([char for char in word if char.isalpha()])]\n",
    "    wordsDict[volume] = textWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#concatenate hypenated words (necessary because of columns in journals) and save to new dictionary\n",
    "fullWordsDict = {}\n",
    "\n",
    "for volume, text in wordsDict.items():\n",
    "\n",
    "    textTokens = []\n",
    "    \n",
    "    for word in wordsDict[volume]:\n",
    "        textTokens.append(word)\n",
    "\n",
    "    count = 0\n",
    "        \n",
    "    for i in range(len(textTokens)):\n",
    "        if textTokens[i].endswith('-'):\n",
    "            count += 1\n",
    "\n",
    "    for i in range(len(textTokens) - count):\n",
    "        if textTokens[i].endswith('-'):\n",
    "            textTokens[i] = textTokens[i][0:-1] + (textTokens[i + 1])\n",
    "            del textTokens[i + 1]\n",
    "\n",
    "    fullWordsDict[volume] = textTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove stopwords and save to new dictionary\n",
    "stoppedWordsDict = {}\n",
    "stopwords = nltk.corpus.stopwords.words(\"English\")\n",
    "\n",
    "for volume, text in fullWordsDict.items():\n",
    "    journalStoppedWords = [word for word in text if word not in stopwords]\n",
    "    stoppedWordsDict[volume] = journalStoppedWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length : 84\n"
     ]
    }
   ],
   "source": [
    "#check that all documents are in dictionary\n",
    "print (\"Length : %d\" % len(stoppedWordsDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write cleaned up text to new directory\n",
    "directory = \"C:/Users/tdahn/Documents/WestVirginiaTXTClean\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "for volume, text in fullWordsDict.items():\n",
    "    cleanString = ' '.join(fullWordsDict[volume])\n",
    "    f = open(directory + \"/\" + volume + \".txt\", \"w\", encoding='utf-8')\n",
    "    f.write(cleanString)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Stay tuned for more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I will be updating this notebook periodically and will make an annoucement via our Twitter account [@CPPMedHistLib](https://twitter.com/CPPHistMedLib).\n",
    "\n",
    "In coming sections I would like to explore more how to work across full corpora, as well as to introduce more complicated concepts such as [sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis), [topic modeling](https://en.wikipedia.org/wiki/Topic_model), [document similarity](https://en.wikipedia.org/wiki/Semantic_similarity) using [vector space modeling](https://en.wikipedia.org/wiki/Vector_space_model), and [data visualization](https://en.wikipedia.org/wiki/Data_visualization).\n",
    "\n",
    "Please feel free to e-mail me directly at [tdahn@collegeofphysicians.org](mailto:tdahn@collegeofphysicians.org) with any questions, comments or corrections. Or fork this on GitHub!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
